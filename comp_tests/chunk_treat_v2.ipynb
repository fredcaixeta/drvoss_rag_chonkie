{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad7045c7",
   "metadata": {},
   "source": [
    "# Voyage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8710ed3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import json\n",
    "import regex as re\n",
    "\n",
    "from typing import Dict\n",
    "\n",
    "# Pega o diretório atual do notebook\n",
    "notebook_dir = os.getcwd() # ou os.path.dirname(__file__) se fosse um script .py\n",
    "\n",
    "# Assume que 'src' está no mesmo nível do notebook ou um nível acima\n",
    "# Ajuste '..' conforme a estrutura do seu projeto\n",
    "project_root = os.path.abspath(os.path.join(notebook_dir, '..')) # Volta um diretório\n",
    "\n",
    "# Se o 'src' estiver diretamente no mesmo nível do notebook:\n",
    "# project_root = notebook_dir\n",
    "\n",
    "# Adiciona o diretório raiz do projeto ao sys.path\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93d36a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.interact_database_sql import get_all_days_content, insert_chunks_emb\n",
    "from src.voyage_emb import get_voyage_embeddings, voyage_rerank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b05d40bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Recuperar apenas content_without_image e content_image_described para todos os registros\n",
    "results = get_all_days_content(fields=['content_without_image', 'content_image_described'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6bbf1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "content = [result['content_image_described'] if result['content_image_described'] != \"\" else result['content_without_image'] for result in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fdcf61ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "123"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53edd1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First import the chunker you want from Chonkie\n",
    "from chonkie import SemanticChunker, RecursiveChunker\n",
    "\n",
    "# Initialize the chunker\n",
    "# chunker = RecursiveChunker()\n",
    "chunker = SemanticChunker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d63af5d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SemanticChunker(model=Model2VecEmbeddings(model_name_or_path=minishlab/potion-base-8M), chunk_size=512, mode=window, threshold=auto, similarity_window=1, min_sentences=1, min_chunk_size=2, return_type=chunks)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57b53de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = []\n",
    "for day in content:\n",
    "    # Chunk some text\n",
    "    _chunks = chunker(day)\n",
    "\n",
    "    # Access chunks\n",
    "    for chunk in _chunks:\n",
    "        #print(f\"Chunk: {chunk.text}\")\n",
    "        chunks.append(chunk.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1d863703",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"For in the interweaving tales of myth and modernity, I sense the emergence of a narrative that is uniquely Veridia's own.\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e0fd604f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"1st Day of Frostfall 1855 - Arrival in the Capital of Veridia\\n\\nPicture - A bustling street market stretches before the grand Assembly House, its pastel facade and colonnaded balconies overlooking the scene. Colorful awnings shade vendors and shoppers, carts laden with produce lining the sunlit avenue.\\n\\n\\nToday marks my arrival in the capital city of Veridia, a place teeming with vibrant cultural heritage and an unyielding commitment to progress. Under Queen Isolde's famed patronage, the arts flourished here. \",\n",
       " \"Walking through the grand avenues, I was captivated by the array of sculptures and paintings, bearing testament to her legacy. My first stop was the regal Assembly House, where the Assembly of Voices, Veridia's main legislative body, convenes. The debate inside, I was told, revolved around initiatives to fulfill Veridia's ambitious goal of using 80% renewable energy by 2050. This pervasive focus on sustainability is palpable, infusing the city's very lifeblood.\\n\\nAs evening crept in, I joined locals indulging in Zelphar stew, a traditional Veridian dish that warms the soul as much as it pleases the palate. Each spoonful was a blend of flavors, rich and comforting after my long journey.\\n\\n####\",\n",
       " \"8th Day of Frostfall 1855 - Exploring the Takron Valley\\n\\nI've ventured into the famed Takron Valley today, laden with a palpable sense of discovery. Here lies the secret hoard of the rare Bluefire Opal, a gemstone that glimmers like the dawn breaking over the mountains. Within the valley, miners worked tirelessly, whispering of its unique beauty and unparalleled worth.\\n\\nIn the evening, I was fortunate enough to witness the excitement of local traditions. \"]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "85e0f151",
   "metadata": {},
   "outputs": [],
   "source": [
    "old_size = 0\n",
    "for chunk in chunks:\n",
    "    new_size = len(chunk)\n",
    "    if new_size > old_size:\n",
    "        max_size = new_size\n",
    "    else:\n",
    "        max_size = old_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7456bbac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "121"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aca6a1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks_emb: list[dict] = [{'chunk': chunk, 'embedding': ''} for chunk in chunks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "59cfb2c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'chunk': \"1st Day of Frostfall 1855 - Arrival in the Capital of Veridia\\n\\nPicture - A bustling street market stretches before the grand Assembly House, its pastel facade and colonnaded balconies overlooking the scene. Colorful awnings shade vendors and shoppers, carts laden with produce lining the sunlit avenue.\\n\\n\\nToday marks my arrival in the capital city of Veridia, a place teeming with vibrant cultural heritage and an unyielding commitment to progress. Under Queen Isolde's famed patronage, the arts flourished here. \",\n",
       "  'embedding': ''},\n",
       " {'chunk': \"Walking through the grand avenues, I was captivated by the array of sculptures and paintings, bearing testament to her legacy. My first stop was the regal Assembly House, where the Assembly of Voices, Veridia's main legislative body, convenes. The debate inside, I was told, revolved around initiatives to fulfill Veridia's ambitious goal of using 80% renewable energy by 2050. This pervasive focus on sustainability is palpable, infusing the city's very lifeblood.\\n\\nAs evening crept in, I joined locals indulging in Zelphar stew, a traditional Veridian dish that warms the soul as much as it pleases the palate. Each spoonful was a blend of flavors, rich and comforting after my long journey.\\n\\n####\",\n",
       "  'embedding': ''},\n",
       " {'chunk': \"8th Day of Frostfall 1855 - Exploring the Takron Valley\\n\\nI've ventured into the famed Takron Valley today, laden with a palpable sense of discovery. Here lies the secret hoard of the rare Bluefire Opal, a gemstone that glimmers like the dawn breaking over the mountains. Within the valley, miners worked tirelessly, whispering of its unique beauty and unparalleled worth.\\n\\nIn the evening, I was fortunate enough to witness the excitement of local traditions. \",\n",
       "  'embedding': ''}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks_emb[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "52d2841c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Make embeddings\n",
    "for chunk in chunks_emb:\n",
    "    # Get the chunk text to encode\n",
    "    chunk_text = chunk['chunk']\n",
    "    # Generate the embedding for this specific chunk\n",
    "    # Note: model.encode usually takes a list of strings, even for a single string,\n",
    "    # and returns a list of embeddings. So, we get the first (and only) embedding.\n",
    "    \n",
    "    # Model2Vec\n",
    "    # embedding = model.encode(chunk_text)\n",
    "    \n",
    "    # Voyage\n",
    "    embedding = get_voyage_embeddings(chunk_text)\n",
    "    time.sleep(0.05)\n",
    "    # Assign the generated embedding to the 'embedding' key\n",
    "    chunk['embedding'] = embedding\n",
    "    \n",
    "\n",
    "# # Make sequences of token embeddings\n",
    "# token_embeddings = model.encode_as_sequence([\"It's dangerous to go alone!\", \"It's a secret to everybody.\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a11abc05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dados salvos com sucesso no SQLite!\n"
     ]
    }
   ],
   "source": [
    "insert_chunks_emb(chunks_emb=chunks_emb)\n",
    "\n",
    "print(\"Dados salvos com sucesso no SQLite!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d9557e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cosine_similarity(vec1: np.ndarray, vec2: np.ndarray) -> float:\n",
    "    \"\"\"Calcula a similaridade de cosseno entre dois vetores.\"\"\"\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    norm_vec1 = np.linalg.norm(vec1)\n",
    "    norm_vec2 = np.linalg.norm(vec2)\n",
    "    \n",
    "    if norm_vec1 == 0 or norm_vec2 == 0:\n",
    "        return 0.0 # Evita divisão por zero\n",
    "        \n",
    "    return dot_product / (norm_vec1 * norm_vec2)\n",
    "\n",
    "def semantic_search(query: str, indexed_chunks: list[dict], model = None, k = 10) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Realiza uma pesquisa de similaridade semântica.\n",
    "\n",
    "    Args:\n",
    "        query (str): A string de consulta.\n",
    "        indexed_chunks (list[dict]): A lista de dicionários de chunks com embeddings.\n",
    "        model: O modelo usado para gerar embeddings.\n",
    "\n",
    "    Returns:\n",
    "        list[dict]: Uma lista de dicionários de chunks, ordenados por similaridade\n",
    "                    (maior primeiro), incluindo a pontuação de similaridade.\n",
    "    \"\"\"\n",
    "    # 1. Gerar o embedding da consulta\n",
    "    query_embedding = get_voyage_embeddings(query)\n",
    "    #print(f\"\\nEmbedding da Consulta ('{query}'): {query_embedding}\")\n",
    "\n",
    "    results = []\n",
    "    # 2. Calcular similaridade para cada chunk\n",
    "    for item in indexed_chunks:\n",
    "        chunk_text = item['chunk']\n",
    "        chunk_embedding = item['embedding']\n",
    "        \n",
    "        # Certifique-se de que o embedding do chunk também é um array numpy\n",
    "        # (se o seu modelo já retorna numpy arrays, isso pode ser redundante)\n",
    "        if not isinstance(chunk_embedding, np.ndarray):\n",
    "             chunk_embedding = np.array(chunk_embedding)\n",
    "\n",
    "        similarity = calculate_cosine_similarity(query_embedding, chunk_embedding)\n",
    "        \n",
    "        results.append({\n",
    "            'chunk': chunk_text,\n",
    "            'similarity': similarity,\n",
    "            'embedding': chunk_embedding # Opcional, para debug\n",
    "        })\n",
    "    \n",
    "    # 3. Ordenar os resultados pela similaridade (decrescente)\n",
    "    results.sort(key=lambda x: x['similarity'], reverse=True)\n",
    "    \n",
    "    return results[:k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead0e0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Inicializa o cliente Gemini API ---\n",
    "from src.classe_gemini import GeminiApiClient\n",
    "# Certifique-se de que a variável de ambiente 'GOOGLE_API_KEY' está definida com sua chave de API\n",
    "try:\n",
    "    api_key = os.environ.get(\"GOOGLE_API_KEY\")\n",
    "    if not api_key:\n",
    "        raise ValueError(\"A variável de ambiente 'GOOGLE_API_KEY' não está definida.\")\n",
    "    \n",
    "    gemini_client = GeminiApiClient(api_key=api_key)\n",
    "except ValueError as e:\n",
    "    print(f\"Erro de configuração da API: {e}\")\n",
    "    exit() # Encerra o programa se a chave da API não estiver configurada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a62515b",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file = r\"C:\\Users\\fuedj\\Documents\\Code\\RAG_Dr_Voss_v2\\drvossv2\\data\\unit_qa.json\"\n",
    "\n",
    "with open(json_file, mode='r', encoding='utf-8') as jf:\n",
    "    jsonfile: Dict = json.load(jf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12872b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#jsonfile = {\"How does the doctor look like?\":\"Hair\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d895b1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers_dict = []\n",
    "k = 30\n",
    "\n",
    "for query, answer in jsonfile.items():\n",
    "    reranked = []\n",
    "    \n",
    "    search_results = semantic_search(query, chunks_emb, k=k)\n",
    "    txt_results = []\n",
    "    \n",
    "    for chunk in search_results:\n",
    "        txt_results.append(chunk['chunk'])\n",
    "    \n",
    "    sorted_rows = voyage_rerank(\n",
    "        query=query,\n",
    "        documents=txt_results[:k]\n",
    "        )    \n",
    "    \n",
    "    top_3 = sorted_rows[:10]  # Já está ordenado por relevance_score (maior para menor)\n",
    "    \n",
    "    # Extrair apenas o texto dos documentos\n",
    "    top_3_texto = [doc for doc, score, index in top_3]\n",
    "    \n",
    "    for i, doc in enumerate(top_3_texto, 1):\n",
    "        reranked.append(doc)\n",
    "        \n",
    "    # --- Chamada da API ---\n",
    "    \n",
    "    model_name = 'gemini-1.5-pro' # Ou 'gemini-1.5-pro' se preferir um modelo mais potente\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "        You are answering a question from a fantasy world in a travel log journey of Doctor Voss, a woman visiting the capital of Veridia.\n",
    "        Question: {query}\n",
    "        Here is the context to help you answer: {reranked}.\n",
    "        Bring the answer **only**. Example: 'The Veridian's Skys were blue most days.'\n",
    "        If you don't know the answer, respond: 'NTD' - meaning nothing to disclosure.\n",
    "        If you are not sure, respond: 'NS' - meaning not sure.\n",
    "        \"\"\"\n",
    "        \n",
    "    prompt_parts = [\n",
    "        {\"text\": f\"{prompt}\"}\n",
    "    ]\n",
    "    # Chama o método da classe GeminiApiClient\n",
    "    response_data = gemini_client.generate_multimodal_content(model_name, prompt_parts)\n",
    "\n",
    "    # Extrai o texto da resposta usando o método da classe\n",
    "    generated_text = gemini_client.extract_text_from_response(response_data)\n",
    "\n",
    "    if generated_text:\n",
    "        print(f\"********************************\\nQuery: {query}\")\n",
    "        print(f\"Gemini: {generated_text}\")\n",
    "        print(f\"Actual Answer: {answer}\")\n",
    "        print(\"--------------------------------\")\n",
    "        \n",
    "        answers_dict.append({\n",
    "            'query': query,\n",
    "            'llm_answer': generated_text,\n",
    "            'actual_answer': answer\n",
    "        })\n",
    "    else:\n",
    "        print(\"\\nNão foi possível extrair texto da resposta do Gemini.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b110961",
   "metadata": {},
   "outputs": [],
   "source": [
    "reranked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478e94ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'C:\\Users\\fuedj\\Documents\\Code\\RAG_Dr_Voss_v2\\drvossv2\\data\\answers_dict.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(answers_dict, f, ensure_ascii=False, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
