{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "19aa73db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import json\n",
    "import regex as re\n",
    "\n",
    "from typing import Dict\n",
    "\n",
    "# Pega o diretório atual do notebook\n",
    "notebook_dir = os.getcwd() # ou os.path.dirname(__file__) se fosse um script .py\n",
    "\n",
    "# Assume que 'src' está no mesmo nível do notebook ou um nível acima\n",
    "# Ajuste '..' conforme a estrutura do seu projeto\n",
    "project_root = os.path.abspath(os.path.join(notebook_dir, '..')) # Volta um diretório\n",
    "\n",
    "# Se o 'src' estiver diretamente no mesmo nível do notebook:\n",
    "# project_root = notebook_dir\n",
    "\n",
    "# Adiciona o diretório raiz do projeto ao sys.path\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d13ddfa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.interact_database_sql import get_all_days_content, insert_chunks_emb, retrieve_chunks_emb\n",
    "from src.voyage_emb import get_voyage_embeddings, voyage_rerank\n",
    "from src.model2vec_emb import load_model2vec, get_model2vec_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "a8e93122",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model2vec_model = load_model2vec(embedder=\"minishlab/potion-base-32M\")\n",
    "model2vec_model = load_model2vec(embedder=\"minishlab/potion-multilingual-128M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7079bc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5a881f6091e4c168a52af6bf3d70e0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/278 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e29b972c104240d59a96db4ce5c19efa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       ".gitattributes:   0%|          | 0.00/1.52k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c6a6adac02a4b1fb0cc89c4607710fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/492k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "440887fee4344eb4b862722b78ff00e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.43k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f0588d92e32416da94f6ac60d867af0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/134 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33e509047f3340d6bb59fa2e84ec07ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.onnx:   0%|          | 0.00/129M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "'tokenizers.Tokenizer' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[84]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(chunker, LateChunker):\n\u001b[32m     14\u001b[39m     whole_document = \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.join(content)\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m     batch_chunks = \u001b[43mchunker\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwhole_document\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m     chunks = []\n\u001b[32m     17\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m batch_chunks:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\fuedj\\Documents\\Code\\RAG_Dr_Voss_v2\\drvossv2\\.venv\\Lib\\site-packages\\chonkie\\chunker\\base.py:48\u001b[39m, in \u001b[36mBaseChunker.__call__\u001b[39m\u001b[34m(self, text, show_progress)\u001b[39m\n\u001b[32m     36\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Call the chunker with the given arguments.\u001b[39;00m\n\u001b[32m     37\u001b[39m \n\u001b[32m     38\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     45\u001b[39m \n\u001b[32m     46\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     47\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     49\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text, Sequence):\n\u001b[32m     50\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.chunk_batch(text, show_progress)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\fuedj\\Documents\\Code\\RAG_Dr_Voss_v2\\drvossv2\\.venv\\Lib\\site-packages\\chonkie\\chunker\\late.py:130\u001b[39m, in \u001b[36mLateChunker.chunk\u001b[39m\u001b[34m(self, text)\u001b[39m\n\u001b[32m    126\u001b[39m \u001b[38;5;66;03m# This would first call upon the _recursive_chunk method\u001b[39;00m\n\u001b[32m    127\u001b[39m \u001b[38;5;66;03m# and then use the embedding model to get the token token_embeddings\u001b[39;00m\n\u001b[32m    128\u001b[39m \u001b[38;5;66;03m# Lastly, we would combine the methods together to create the LateChunk objects\u001b[39;00m\n\u001b[32m    129\u001b[39m chunks = \u001b[38;5;28mself\u001b[39m._recursive_chunk(text)\n\u001b[32m--> \u001b[39m\u001b[32m130\u001b[39m token_embeddings = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43membedding_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43membed_as_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    132\u001b[39m \u001b[38;5;66;03m# Get the token_counts for all the chunks\u001b[39;00m\n\u001b[32m    133\u001b[39m token_counts = [c.token_count \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m chunks]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\fuedj\\Documents\\Code\\RAG_Dr_Voss_v2\\drvossv2\\.venv\\Lib\\site-packages\\chonkie\\embeddings\\sentence_transformer.py:76\u001b[39m, in \u001b[36mSentenceTransformerEmbeddings.embed_as_tokens\u001b[39m\u001b[34m(self, text)\u001b[39m\n\u001b[32m     73\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m np.array([])\n\u001b[32m     75\u001b[39m \u001b[38;5;66;03m# Use the model's tokenizer to encode the text\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m76\u001b[39m encodings = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m[\u001b[33m\"\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     78\u001b[39m max_seq_length = \u001b[38;5;28mself\u001b[39m.max_seq_length\n\u001b[32m     79\u001b[39m token_splits = []\n",
      "\u001b[31mTypeError\u001b[39m: 'tokenizers.Tokenizer' object is not callable"
     ]
    }
   ],
   "source": [
    "# 1. Recuperar apenas content_without_image e content_image_described para todos os registros\n",
    "results = get_all_days_content(fields=['content_without_image', 'content_image_described'])\n",
    "content = [result['content_image_described'] if result['content_image_described'] != \"\" else result['content_without_image'] for result in results]\n",
    "\n",
    "# First import the chunker you want from Chonkie\n",
    "from chonkie import SemanticChunker, RecursiveChunker, LateChunker\n",
    "\n",
    "# # Initialize the chunker\n",
    "# chunker = RecursiveChunker()\n",
    "# chunker = SemanticChunker()\n",
    "#chunker = LateChunker(embedding_model=\"minishlab/potion-multilingual-128M\")\n",
    "chunker = LateChunker(embedding_model=\"minishlab/potion-base-32M\")\n",
    "chunker()\n",
    "if isinstance(chunker, LateChunker):\n",
    "    whole_document = \"\\n\".join(content)\n",
    "    batch_chunks = chunker(whole_document)\n",
    "    chunks = []\n",
    "    for chunk in batch_chunks:\n",
    "        chunks.append(chunk.text)\n",
    "\n",
    "else:\n",
    "    chunks = []\n",
    "    for day in content:\n",
    "        # Chunk some text\n",
    "        _chunks = chunker(day)\n",
    "\n",
    "        # Access chunks\n",
    "        for chunk in _chunks:\n",
    "            #print(f\"Chunk: {chunk.text}\")\n",
    "            chunks.append(chunk.text)\n",
    "            \n",
    "chunks_emb: list[dict] = [{'chunk': chunk, 'embedding': ''} for chunk in chunks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "ae3f08c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba5509f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nPicture - A colossal stone arch dominates the val'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks_emb[1]['chunk'][:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67cad843",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunks_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847ff2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Make embeddings\n",
    "for chunk in chunks_emb:\n",
    "    # Get the chunk text to encode\n",
    "    chunk_text = chunk['chunk']\n",
    "    # Generate the embedding for this specific chunk\n",
    "    # Note: model.encode usually takes a list of strings, even for a single string,\n",
    "    # and returns a list of embeddings. So, we get the first (and only) embedding.\n",
    "    \n",
    "    # Model2Vec\n",
    "    \n",
    "    embedding = get_model2vec_embeddings(model=model2vec_model, text=chunk_text)\n",
    "    \n",
    "    # Assign the generated embedding to the 'embedding' key\n",
    "    chunk['embedding'] = embedding\n",
    "    \n",
    "\n",
    "# # Make sequences of token embeddings\n",
    "# token_embeddings = model.encode_as_sequence([\"It's dangerous to go alone!\", \"It's a secret to everybody.\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707e6c38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunk['embedding'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ebf9d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cosine_similarity(vec1: np.ndarray, vec2: np.ndarray) -> float:\n",
    "    \"\"\"Calcula a similaridade de cosseno entre dois vetores.\"\"\"\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    norm_vec1 = np.linalg.norm(vec1)\n",
    "    norm_vec2 = np.linalg.norm(vec2)\n",
    "    \n",
    "    if norm_vec1 == 0 or norm_vec2 == 0:\n",
    "        return 0.0 # Evita divisão por zero\n",
    "        \n",
    "    return dot_product / (norm_vec1 * norm_vec2)\n",
    "\n",
    "def semantic_search(query: str, indexed_chunks: list[dict], model = None, k = 10) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Realiza uma pesquisa de similaridade semântica.\n",
    "\n",
    "    Args:\n",
    "        query (str): A string de consulta.\n",
    "        indexed_chunks (list[dict]): A lista de dicionários de chunks com embeddings.\n",
    "        model: O modelo usado para gerar embeddings.\n",
    "\n",
    "    Returns:\n",
    "        list[dict]: Uma lista de dicionários de chunks, ordenados por similaridade\n",
    "                    (maior primeiro), incluindo a pontuação de similaridade.\n",
    "    \"\"\"\n",
    "    # 1. Gerar o embedding da consulta\n",
    "    #model2vec_model = load_model2vec(embedder=\"minishlab/potion-base-32M\")\n",
    "    query_embedding = get_model2vec_embeddings(model=model2vec_model, text=query)\n",
    "    #print(f\"\\nEmbedding da Consulta ('{query}'): {query_embedding}\")\n",
    "\n",
    "    results = []\n",
    "    # 2. Calcular similaridade para cada chunk\n",
    "    for item in indexed_chunks:\n",
    "        chunk_text = item['chunk']\n",
    "        chunk_embedding = item['embedding']\n",
    "        \n",
    "        # Certifique-se de que o embedding do chunk também é um array numpy\n",
    "        # (se o seu modelo já retorna numpy arrays, isso pode ser redundante)\n",
    "        if not isinstance(chunk_embedding, np.ndarray):\n",
    "             chunk_embedding = np.array(chunk_embedding)\n",
    "\n",
    "        similarity = calculate_cosine_similarity(query_embedding, chunk_embedding)\n",
    "        \n",
    "        results.append({\n",
    "            'chunk': chunk_text,\n",
    "            'similarity': similarity,\n",
    "            'embedding': chunk_embedding # Opcional, para debug\n",
    "        })\n",
    "    \n",
    "    # 3. Ordenar os resultados pela similaridade (decrescente)\n",
    "    results.sort(key=lambda x: x['similarity'], reverse=True)\n",
    "    \n",
    "    return results[:k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7e7977",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Inicializa o cliente Gemini API ---\n",
    "from src.classe_gemini import GeminiApiClient\n",
    "# Certifique-se de que a variável de ambiente 'GOOGLE_API_KEY' está definida com sua chave de API\n",
    "try:\n",
    "    api_key = os.environ.get(\"GOOGLE_API_KEY\")\n",
    "    if not api_key:\n",
    "        raise ValueError(\"A variável de ambiente 'GOOGLE_API_KEY' não está definida.\")\n",
    "    \n",
    "    gemini_client = GeminiApiClient(api_key=api_key)\n",
    "except ValueError as e:\n",
    "    print(f\"Erro de configuração da API: {e}\")\n",
    "    exit() # Encerra o programa se a chave da API não estiver configurada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c43161f",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file = r\"C:\\Users\\fuedj\\Documents\\Code\\RAG_Dr_Voss_v2\\drvossv2\\data\\unit_qa_harder_questions.json\"\n",
    "\n",
    "with open(json_file, mode='r', encoding='utf-8') as jf:\n",
    "    jsonfile: Dict = json.load(jf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab337c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "#jsonfile = {\"How does the doctor hair colour?\":\"Hair\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d3e3e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************************\n",
      "Query: Which historical event is commemorated on Veridia's Independence Day?\n",
      "Gemini: The signing of the Treaty of Syth in 1854.\n",
      "\n",
      "Actual Answer: Veridia's Independence Day commemorates the signing of the Treaty of Syth in 1854.\n",
      "--------------------------------\n",
      "********************************\n",
      "Query: Who is Veridia's most famous historical figure?\n",
      "Gemini: Queen Seraphina\n",
      "\n",
      "Actual Answer: Queen Seraphina is Veridia's most famous historical figure.\n",
      "--------------------------------\n",
      "********************************\n",
      "Query: What is the highest point in Veridia?\n",
      "Gemini: Mount Alenor\n",
      "\n",
      "Actual Answer: The highest point in Veridia is Mount Alenor.\n",
      "--------------------------------\n",
      "********************************\n",
      "Query: Which Veridian philosopher is famous for the Theory of Harmonious Existence?\n",
      "Gemini: Theodor Alvyn\n",
      "\n",
      "Actual Answer: Theodor Alvyn is famous for the Theory of Harmonious Existence.\n",
      "--------------------------------\n",
      "********************************\n",
      "Query: What architectural style is prominent in Veridia's historical buildings?\n",
      "Gemini: Celestial Renaissance\n",
      "\n",
      "Actual Answer: Celestial Renaissance is the prominent architectural style in Veridia.\n",
      "--------------------------------\n",
      "********************************\n",
      "Query: What is the name of Veridia’s most prestigious literary prize?\n",
      "Gemini: The Illumina Award\n",
      "\n",
      "Actual Answer: The Illumina Award is Veridia’s most prestigious literary prize.\n",
      "--------------------------------\n",
      "********************************\n",
      "Query: What historical event does the Veridian Flower Festival commemorate?\n",
      "Gemini: The peace accord of 1723.\n",
      "\n",
      "Actual Answer: The Veridian Flower Festival commemorates the peace accord of 1723.\n",
      "--------------------------------\n",
      "********************************\n",
      "Query: What is the primary focus of Veridia's National Innovation Fund?\n",
      "Gemini: NTD\n",
      "\n",
      "Actual Answer: The primary focus is on sustainable technology and green initiatives.\n",
      "--------------------------------\n",
      "********************************\n",
      "Query: What cultural practice is unique to the Veridian Festival of Lights?\n",
      "Gemini: Synchronized drone light displays.\n",
      "\n",
      "Actual Answer: The lighting of spirit lanterns to honor ancestors is unique to the Festival of Lights.\n",
      "--------------------------------\n",
      "********************************\n",
      "Query: What year did Veridia join the United Cosmic Alliance?\n",
      "Gemini: 2030\n",
      "\n",
      "Actual Answer: Veridia joined the United Cosmic Alliance in 2045.\n",
      "--------------------------------\n",
      "********************************\n",
      "Query: Which landmark in Veridia is considered a wonder of the ancient world?\n",
      "Gemini: The Starlit Towers\n",
      "\n",
      "Actual Answer: The Starlit Towers are considered a wonder of the ancient world in Veridia.\n",
      "--------------------------------\n",
      "********************************\n",
      "Query: Who is the Veridian novelist known for the 'Chronicles of the Shadows' series?\n",
      "Gemini: Lia Theron\n",
      "\n",
      "Actual Answer: Lia Theron is known for the 'Chronicles of the Shadows' series.\n",
      "--------------------------------\n",
      "********************************\n",
      "Query: What is the environmental initiative called that Veridia launched in 2020?\n",
      "Gemini: NTD\n",
      "\n",
      "Actual Answer: The initiative is called the Veridian Green Horizon.\n",
      "--------------------------------\n",
      "********************************\n",
      "Query: Which invention is attributed to Veridian engineer Tyra Kael?\n",
      "Gemini: NTD\n",
      "\n",
      "Actual Answer: The Hyperflux Collider is attributed to Tyra Kael.\n",
      "--------------------------------\n"
     ]
    }
   ],
   "source": [
    "answers_dict = []\n",
    "k = 30\n",
    "\n",
    "#chunks_emb = retrieve_chunks_emb()\n",
    "for query, answer in jsonfile.items():\n",
    "    reranked = []\n",
    "    \n",
    "    search_results = semantic_search(query, chunks_emb, k=k)\n",
    "    txt_results = []\n",
    "    \n",
    "    for chunk in search_results:\n",
    "        txt_results.append(chunk['chunk'])\n",
    "    \n",
    "    sorted_rows = voyage_rerank(\n",
    "        query=query,\n",
    "        documents=txt_results[:k]\n",
    "        )    \n",
    "    \n",
    "    top_3 = sorted_rows[:10]  # Já está ordenado por relevance_score (maior para menor)\n",
    "    \n",
    "    # Extrair apenas o texto dos documentos\n",
    "    top_3_texto = [doc for doc, score, index in top_3]\n",
    "    \n",
    "    for i, doc in enumerate(top_3_texto, 1):\n",
    "        reranked.append(doc)\n",
    "        \n",
    "    # --- Chamada da API ---\n",
    "    \n",
    "    model_name = 'gemini-1.5-flash-8b' # Ou 'gemini-1.5-pro' se preferir um modelo mais potente\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "        You are answering a question from a fantasy world in a travel log journey of Doctor Voss, a woman visiting the capital of Veridia.\n",
    "        Question: {query}\n",
    "        Here is the context to help you answer: {reranked}.\n",
    "        Bring the answer **only**. Example: 'The Veridian's Skys were blue most days.'\n",
    "        If you don't know the answer, respond: 'NTD' - meaning nothing to disclosure.\n",
    "        If you are not sure, respond: 'NS' - meaning not sure.\n",
    "        \"\"\"\n",
    "        \n",
    "    prompt_parts = [\n",
    "        {\"text\": f\"{prompt}\"}\n",
    "    ]\n",
    "    # Chama o método da classe GeminiApiClient\n",
    "    response_data = gemini_client.generate_multimodal_content(model_name, prompt_parts)\n",
    "\n",
    "    # Extrai o texto da resposta usando o método da classe\n",
    "    generated_text = gemini_client.extract_text_from_response(response_data)\n",
    "\n",
    "    if generated_text:\n",
    "        print(f\"********************************\\nQuery: {query}\")\n",
    "        print(f\"Gemini: {generated_text}\")\n",
    "        print(f\"Actual Answer: {answer}\")\n",
    "        print(\"--------------------------------\")\n",
    "        \n",
    "        answers_dict.append({\n",
    "            'query': query,\n",
    "            'llm_answer': generated_text,\n",
    "            'actual_answer': answer\n",
    "        })\n",
    "    else:\n",
    "        print(\"\\nNão foi possível extrair texto da resposta do Gemini.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21c6988",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(r'C:\\Users\\fuedj\\Documents\\Code\\RAG_Dr_Voss_v2\\drvossv2\\data\\answers_dict.json', 'w', encoding='utf-8') as f:\n",
    "#     json.dump(answers_dict, f, ensure_ascii=False, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
