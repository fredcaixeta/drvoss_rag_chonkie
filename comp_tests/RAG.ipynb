{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19aa73db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import json\n",
    "import regex as re\n",
    "\n",
    "from typing import Dict\n",
    "\n",
    "# Pega o diretório atual do notebook\n",
    "notebook_dir = os.getcwd() # ou os.path.dirname(__file__) se fosse um script .py\n",
    "\n",
    "# Assume que 'src' está no mesmo nível do notebook ou um nível acima\n",
    "# Ajuste '..' conforme a estrutura do seu projeto\n",
    "project_root = os.path.abspath(os.path.join(notebook_dir, '..')) # Volta um diretório\n",
    "\n",
    "# Se o 'src' estiver diretamente no mesmo nível do notebook:\n",
    "# project_root = notebook_dir\n",
    "\n",
    "# Adiciona o diretório raiz do projeto ao sys.path\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13ddfa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.interact_database_sql import get_all_days_content, insert_chunks_emb, retrieve_chunks_emb\n",
    "from src.voyage_emb import get_voyage_embeddings, voyage_rerank\n",
    "from src.model2vec_emb import load_model2vec, get_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ebf9d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cosine_similarity(vec1: np.ndarray, vec2: np.ndarray) -> float:\n",
    "    \"\"\"Calcula a similaridade de cosseno entre dois vetores.\"\"\"\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    norm_vec1 = np.linalg.norm(vec1)\n",
    "    norm_vec2 = np.linalg.norm(vec2)\n",
    "    \n",
    "    if norm_vec1 == 0 or norm_vec2 == 0:\n",
    "        return 0.0 # Evita divisão por zero\n",
    "        \n",
    "    return dot_product / (norm_vec1 * norm_vec2)\n",
    "\n",
    "def semantic_search(query: str, indexed_chunks: list[dict], model = None, k = 10) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Realiza uma pesquisa de similaridade semântica.\n",
    "\n",
    "    Args:\n",
    "        query (str): A string de consulta.\n",
    "        indexed_chunks (list[dict]): A lista de dicionários de chunks com embeddings.\n",
    "        model: O modelo usado para gerar embeddings.\n",
    "\n",
    "    Returns:\n",
    "        list[dict]: Uma lista de dicionários de chunks, ordenados por similaridade\n",
    "                    (maior primeiro), incluindo a pontuação de similaridade.\n",
    "    \"\"\"\n",
    "    # 1. Gerar o embedding da consulta\n",
    "    query_embedding = get_voyage_embeddings(query)\n",
    "    #print(f\"\\nEmbedding da Consulta ('{query}'): {query_embedding}\")\n",
    "\n",
    "    results = []\n",
    "    # 2. Calcular similaridade para cada chunk\n",
    "    for item in indexed_chunks:\n",
    "        chunk_text = item['chunk']\n",
    "        chunk_embedding = item['embedding']\n",
    "        \n",
    "        # Certifique-se de que o embedding do chunk também é um array numpy\n",
    "        # (se o seu modelo já retorna numpy arrays, isso pode ser redundante)\n",
    "        if not isinstance(chunk_embedding, np.ndarray):\n",
    "             chunk_embedding = np.array(chunk_embedding)\n",
    "\n",
    "        similarity = calculate_cosine_similarity(query_embedding, chunk_embedding)\n",
    "        \n",
    "        results.append({\n",
    "            'chunk': chunk_text,\n",
    "            'similarity': similarity,\n",
    "            'embedding': chunk_embedding # Opcional, para debug\n",
    "        })\n",
    "    \n",
    "    # 3. Ordenar os resultados pela similaridade (decrescente)\n",
    "    results.sort(key=lambda x: x['similarity'], reverse=True)\n",
    "    \n",
    "    return results[:k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b7e7977",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Inicializa o cliente Gemini API ---\n",
    "from src.classe_gemini import GeminiApiClient\n",
    "# Certifique-se de que a variável de ambiente 'GOOGLE_API_KEY' está definida com sua chave de API\n",
    "try:\n",
    "    api_key = os.environ.get(\"GOOGLE_API_KEY\")\n",
    "    if not api_key:\n",
    "        raise ValueError(\"A variável de ambiente 'GOOGLE_API_KEY' não está definida.\")\n",
    "    \n",
    "    gemini_client = GeminiApiClient(api_key=api_key)\n",
    "except ValueError as e:\n",
    "    print(f\"Erro de configuração da API: {e}\")\n",
    "    exit() # Encerra o programa se a chave da API não estiver configurada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c43161f",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file = r\"C:\\Users\\fuedj\\Documents\\Code\\RAG_Dr_Voss_v2\\drvossv2\\data\\unit_qa.json\"\n",
    "\n",
    "with open(json_file, mode='r', encoding='utf-8') as jf:\n",
    "    jsonfile: Dict = json.load(jf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab337c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "jsonfile = {\"How does the doctor look like?\":\"Hair\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d3e3e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************************\n",
      "Query: How does the doctor look like?\n",
      "Gemini: Dr. Voss is fair-skinned with auburn hair styled in a soft wave. She wears a teal wrap dress and a delicate necklace.\n",
      "\n",
      "Actual Answer: Hair\n",
      "--------------------------------\n"
     ]
    }
   ],
   "source": [
    "answers_dict = []\n",
    "k = 30\n",
    "\n",
    "chunks_emb = retrieve_chunks_emb()\n",
    "for query, answer in jsonfile.items():\n",
    "    reranked = []\n",
    "    \n",
    "    search_results = semantic_search(query, chunks_emb, k=k)\n",
    "    txt_results = []\n",
    "    \n",
    "    for chunk in search_results:\n",
    "        txt_results.append(chunk['chunk'])\n",
    "    \n",
    "    sorted_rows = voyage_rerank(\n",
    "        query=query,\n",
    "        documents=txt_results[:k]\n",
    "        )    \n",
    "    \n",
    "    top_3 = sorted_rows[:10]  # Já está ordenado por relevance_score (maior para menor)\n",
    "    \n",
    "    # Extrair apenas o texto dos documentos\n",
    "    top_3_texto = [doc for doc, score, index in top_3]\n",
    "    \n",
    "    for i, doc in enumerate(top_3_texto, 1):\n",
    "        reranked.append(doc)\n",
    "        \n",
    "    # --- Chamada da API ---\n",
    "    \n",
    "    model_name = 'gemini-1.5-flash-8b' # Ou 'gemini-1.5-pro' se preferir um modelo mais potente\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "        You are answering a question from a fantasy world in a travel log journey of Doctor Voss, a woman visiting the capital of Veridia.\n",
    "        Question: {query}\n",
    "        Here is the context to help you answer: {reranked}.\n",
    "        Bring the answer **only**. Example: 'The Veridian's Skys were blue most days.'\n",
    "        If you don't know the answer, respond: 'NTD' - meaning nothing to disclosure.\n",
    "        If you are not sure, respond: 'NS' - meaning not sure.\n",
    "        \"\"\"\n",
    "        \n",
    "    prompt_parts = [\n",
    "        {\"text\": f\"{prompt}\"}\n",
    "    ]\n",
    "    # Chama o método da classe GeminiApiClient\n",
    "    response_data = gemini_client.generate_multimodal_content(model_name, prompt_parts)\n",
    "\n",
    "    # Extrai o texto da resposta usando o método da classe\n",
    "    generated_text = gemini_client.extract_text_from_response(response_data)\n",
    "\n",
    "    if generated_text:\n",
    "        print(f\"********************************\\nQuery: {query}\")\n",
    "        print(f\"Gemini: {generated_text}\")\n",
    "        print(f\"Actual Answer: {answer}\")\n",
    "        print(\"--------------------------------\")\n",
    "        \n",
    "        answers_dict.append({\n",
    "            'query': query,\n",
    "            'llm_answer': generated_text,\n",
    "            'actual_answer': answer\n",
    "        })\n",
    "    else:\n",
    "        print(\"\\nNão foi possível extrair texto da resposta do Gemini.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f21c6988",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(r'C:\\Users\\fuedj\\Documents\\Code\\RAG_Dr_Voss_v2\\drvossv2\\data\\answers_dict.json', 'w', encoding='utf-8') as f:\n",
    "#     json.dump(answers_dict, f, ensure_ascii=False, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
