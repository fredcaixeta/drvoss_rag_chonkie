{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "49ac4252",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import json\n",
    "import regex as re\n",
    "\n",
    "from typing import Dict\n",
    "\n",
    "# Pega o diretório atual do notebook\n",
    "notebook_dir = os.getcwd() # ou os.path.dirname(__file__) se fosse um script .py\n",
    "\n",
    "# Assume que 'src' está no mesmo nível do notebook ou um nível acima\n",
    "# Ajuste '..' conforme a estrutura do seu projeto\n",
    "project_root = os.path.abspath(os.path.join(notebook_dir, '..')) # Volta um diretório\n",
    "\n",
    "# Se o 'src' estiver diretamente no mesmo nível do notebook:\n",
    "# project_root = notebook_dir\n",
    "\n",
    "# Adiciona o diretório raiz do projeto ao sys.path\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7716e00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.classe_gemini import GeminiApiClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "42a17c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import List, Dict\n",
    "import statistics\n",
    "\n",
    "# Suponha que gemini_client esteja definido com generate_multimodal_content e extract_text_from_response\n",
    "\n",
    "def evaluate_answers(\n",
    "    answers_dict: List[Dict[str, str]],\n",
    "    model_name: str = \"gemini-1.5-pro\"\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Avalia as respostas em answers_dict usando a API do Gemini para calcular similaridade semântica.\n",
    "\n",
    "    Args:\n",
    "        answers_dict (List[Dict[str, str]]): Lista de dicionários com 'query', 'llm_answer', 'actual_answer'.\n",
    "        gemini_client: Cliente da API Gemini com métodos generate_multimodal_content e extract_text_from_response.\n",
    "        model_name (str): Modelo Gemini a ser usado (padrão: 'gemini-1.5-pro').\n",
    "\n",
    "    Returns:\n",
    "        Dict: Resultados da avaliação, incluindo métricas e detalhes por query.\n",
    "    \"\"\"\n",
    "    \n",
    "        # --- Inicializa o cliente Gemini API ---\n",
    "    \n",
    "    try:\n",
    "        api_key = os.environ.get(\"GOOGLE_API_KEY\")\n",
    "        if not api_key:\n",
    "            raise ValueError(\"A variável de ambiente 'GOOGLE_API_KEY' não está definida.\")\n",
    "        \n",
    "        gemini_client = GeminiApiClient(api_key=api_key)\n",
    "    except ValueError as e:\n",
    "        print(f\"Erro de configuração da API: {e}\")\n",
    "        exit() # Encerra o programa se a chave da API não estiver configurada\n",
    "        \n",
    "    evaluation_results = {\n",
    "        \"evaluations\": [],\n",
    "        \"average_similarity\": 0.0,\n",
    "        \"exact_match_count\": 0,\n",
    "        \"total_queries\": len(answers_dict)\n",
    "    }\n",
    "    \n",
    "    for entry in answers_dict:\n",
    "        query = entry.get('query', '')\n",
    "        llm_answer = entry.get('llm_answer', '')\n",
    "        actual_answer = entry.get('actual_answer', '')\n",
    "\n",
    "        # Verificar acurácia exata (case-insensitive, ignorando espaços)\n",
    "        # is_exact_match = (\n",
    "        #     llm_answer.strip().lower() == actual_answer.strip().lower()\n",
    "        #     if llm_answer and actual_answer else False\n",
    "        # )\n",
    "        # if is_exact_match:\n",
    "        #     evaluation_results[\"exact_match_count\"] += 1\n",
    "\n",
    "        # Montar o prompt para o Gemini avaliar similaridade\n",
    "        \n",
    "        \n",
    "        system_prompt = f\"\"\"\n",
    "            You are an evaluator in a fantasy world context. Your task is to compare two answers to a question and provide a semantic similarity score between 0 and 1, where:\n",
    "            - 1 means the answers are semantically identical or convey the same meaning.\n",
    "            - 0 means the answers are completely different.\n",
    "            - Partial similarity should be scored between 0 and 1 (e.g., 0.8 for very similar answers with minor differences).\n",
    "\n",
    "            Input format:\n",
    "            - Question: {query}\n",
    "            - Generated Answer: {llm_answer}\n",
    "            - Expected Answer: {actual_answer}\n",
    "\n",
    "            Output format:\n",
    "            - Similarity Score: [number between 0 and 1]\n",
    "            - Explanation: [brief explanation of the similarity score]\n",
    "        \"\"\"\n",
    "\n",
    "        prompt = system_prompt\n",
    "\n",
    "        prompt_parts = [{\"text\": prompt}]\n",
    "\n",
    "        # Chamar a API do Gemini\n",
    "        try:\n",
    "            response_data = gemini_client.generate_multimodal_content(model_name, prompt_parts)\n",
    "            #print(response_data)\n",
    "            evaluation_text = gemini_client.extract_text_from_response(response_data)\n",
    "\n",
    "            # Extrair pontuação de similaridade e explicação\n",
    "            similarity_score = 0.0\n",
    "            explanation = \"No explanation provided.\"\n",
    "            if evaluation_text:\n",
    "                lines = evaluation_text.split('\\n')\n",
    "                for line in lines:\n",
    "                    if line.startswith(\"Similarity Score:\"):\n",
    "                        try:\n",
    "                            similarity_score = float(line.split(\":\")[1].strip())\n",
    "                        except (ValueError, IndexError):\n",
    "                            print(f\"Erro ao extrair pontuação para query: {query}\")\n",
    "                    elif line.startswith(\"Explanation:\"):\n",
    "                        explanation = line.split(\":\", 1)[1].strip()\n",
    "\n",
    "            evaluation_results[\"evaluations\"].append({\n",
    "                \"query\": query,\n",
    "                \"llm_answer\": llm_answer,\n",
    "                \"actual_answer\": actual_answer,\n",
    "                \"similarity_score\": similarity_score,\n",
    "                \"explanation\": explanation\n",
    "            })\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao avaliar query '{query}': {e}\")\n",
    "            evaluation_results[\"evaluations\"].append({\n",
    "                \"query\": query,\n",
    "                \"llm_answer\": llm_answer,\n",
    "                \"actual_answer\": actual_answer,\n",
    "                \"similarity_score\": 0.0,\n",
    "                \"explanation\": f\"Erro na avaliação: {str(e)}\"\n",
    "            })\n",
    "\n",
    "    # Calcular média dos scores de similaridade\n",
    "    if evaluation_results[\"evaluations\"]:\n",
    "        scores = [e[\"similarity_score\"] for e in evaluation_results[\"evaluations\"]]\n",
    "        evaluation_results[\"average_similarity\"] = statistics.mean(scores) if scores else 0.0\n",
    "\n",
    "    return evaluation_results\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f76cfbc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar answers_dict de um arquivo JSON (ou usar diretamente a lista)\n",
    "try:\n",
    "    with open(r'C:\\Users\\fuedj\\Documents\\Code\\RAG_Dr_Voss_v2\\drvossv2\\data\\answers_dict.json', 'r', encoding='utf-8') as f:\n",
    "        answers_dict = json.load(f)\n",
    "except FileNotFoundError:\n",
    "    print(\"Arquivo answers_dict.json não encontrado. Usando dados de exemplo.\")\n",
    "    answers_dict = [\n",
    "        {\n",
    "            'query': \"Who is the current Grand Chancellor of Veridia?\",\n",
    "            'llm_answer': \"Queen Isolde\",\n",
    "            'actual_answer': \"Queen Isolde\"\n",
    "        },\n",
    "        {\n",
    "            'query': \"What is Zelphar stew?\",\n",
    "            'llm_answer': \"A traditional Veridian dish that warms the soul\",\n",
    "            'actual_answer': \"A traditional Veridian dish\"\n",
    "        }\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a839275",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avaliar as respostas\n",
    "evaluation_results = evaluate_answers(answers_dict, model_name=\"gemini-1.5-pro\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5982be4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Resultados da Avaliação:\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[51]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Exibir resultados\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mResultados da Avaliação:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m eval_entry \u001b[38;5;129;01min\u001b[39;00m \u001b[43mevaluation_results\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mevaluations\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m:\n\u001b[32m      6\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mQuery: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00meval_entry[\u001b[33m'\u001b[39m\u001b[33mquery\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      7\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLLM Answer: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00meval_entry[\u001b[33m'\u001b[39m\u001b[33mllm_answer\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mTypeError\u001b[39m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "\n",
    "# Exibir resultados\n",
    "print(\"\\nResultados da Avaliação:\")\n",
    "\n",
    "\n",
    "for eval_entry in evaluation_results[\"evaluations\"]:\n",
    "    print(f\"Query: {eval_entry['query']}\")\n",
    "    print(f\"LLM Answer: {eval_entry['llm_answer']}\")\n",
    "    print(f\"Actual Answer: {eval_entry['actual_answer']}\")\n",
    "    print(f\"Similarity Score: {eval_entry['similarity_score']}\")\n",
    "    print(f\"Explanation: {eval_entry['explanation']}\")\n",
    "    print(f\"Exact Match: {eval_entry['exact_match']}\")\n",
    "    print(\"---------------------------------\")\n",
    "\n",
    "# Opcional: Salvar resultados da avaliação em um arquivo JSON\n",
    "with open('evaluation_results.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(evaluation_results, f, ensure_ascii=False, indent=4)\n",
    "print(\"Resultados da avaliação salvos em evaluation_results.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b7e04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73cf352d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Similarity Score: 0.2\n",
      "- Explanation: The expected answer is extremely concise and only mentions \"Hair.\"  While the generated answer *does* contain information about the doctor's hair (auburn/reddish-brown, soft wave), it provides much more detail about the doctor's appearance than requested.  Since the question asked what the doctor *looks like* generally, and the expected answer seems to be focusing on a specific feature (perhaps as a shorthand or in a testing scenario where only the hair color or style was relevant), the generated response, while correct in describing the hair, is mostly superfluous information. Therefore, the similarity is low.  Had the question been more specific about hair, or the expected answer included more features, the score would be higher.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response_data = {'candidates': [{'content': {'parts': [{'text': '- Similarity Score: 0.2\\n- Explanation: The expected answer is extremely concise and only mentions \"Hair.\"  While the generated answer *does* contain information about the doctor\\'s hair (auburn/reddish-brown, soft wave), it provides much more detail about the doctor\\'s appearance than requested.  Since the question asked what the doctor *looks like* generally, and the expected answer seems to be focusing on a specific feature (perhaps as a shorthand or in a testing scenario where only the hair color or style was relevant), the generated response, while correct in describing the hair, is mostly superfluous information. Therefore, the similarity is low.  Had the question been more specific about hair, or the expected answer included more features, the score would be higher.\\n'}], 'role': 'model'}, 'finishReason': 'STOP', 'avgLogprobs': -0.4062601089477539}], 'usageMetadata': {'promptTokenCount': 188, 'candidatesTokenCount': 160, 'totalTokenCount': 348, 'promptTokensDetails': [{'modality': 'TEXT', 'tokenCount': 188}], 'candidatesTokensDetails': [{'modality': 'TEXT', 'tokenCount': 160}]}, 'modelVersion': 'gemini-1.5-pro-002', 'responseId': 'z8FUaNboEqqO7dcP7s2EyAY'}\n",
    "\n",
    "candidates = response_data.get(\"candidates\", [])\n",
    "if candidates:\n",
    "    content = candidates[0].get(\"content\", {})\n",
    "    parts = content.get(\"parts\", [])\n",
    "    if parts:\n",
    "        # Assumindo que a resposta de texto gerada estará na primeira 'part'\n",
    "        # e que é um campo 'text'. Pode precisar de mais lógica se a resposta for complexa.\n",
    "        print(parts[0].get(\"text\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
